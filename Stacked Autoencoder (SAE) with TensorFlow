"""
Creation Date: 12-12-2017
Created By: Alok Kumar
Dataset: MNIST Dataset
Data Source: http://yann.lecun.com/exdb/mnist/
Dataset Description: A database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples.
Dataset File Count: 4
Dataset Files: train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz
Modelling Method: Stacked Autoencoder (SAE)
Modelling Agenda: To reduce dimensions of the dataset by decreasing attribute size, still retaining pattern & features.
Dependancies: Python, Numpy, Pandas & TensorFlow.
"""

# Importing Libraries:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

# Fetching Dataset:
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(".../MNIST_data/", one_hot=True)

# Creating parameters:
num_inputs = 784
neurons_hid1 = 392
neurons_hid2 = 196
neurons_hid3 = neurons_hid1
num_outputs = num_inputs
learning_rate = 0.01
actf = tf.nn.relu

# Creating Placeholders:
X = tf.placeholder(tf.float32), shape=[None, num_inputs])
initializer = tf.variance_scaling_initializer()

# Assigning weights:
w1 = tf.Variable(initializer([num_inputs, neurons_hid1]), dtype=tf.float32)
w2 = tf.Variable(initializer([neurons_hid1, neurons_hid2]), dtype=tf.float32)
w3 = tf.Variable(initializer([neurons_hid2, neurons_hid3]), dtype=tf.float32)
w4 = tf.Variable(initializer([neurons_hid3, num_outputs]), dtype=tf.float32)

# Creating Bias:
b1 = tf.Variable(tf.zeroes(neurons_hid1))
b2 = tf.Variable(tf.zeroes(neurons_hid2))
b3 = tf.Variable(tf.zeroes(neurons_hid3))
b4 = tf.Variable(tf.zeroes(num_outputs))

# Creating Hidden Layers:
hid_layer1 = actf(tf.matmul(X, w1) + b1)
hid_layer2 = actf(tf.matmul(hid_layer1, w2) + b2)
hid_layer3 = actf(tf.matmul(hid_layer2, w3) + b3)
output_layer = actf(tf.matmul(hid_layer3, w4) + b4)

# Loss Function:
loss = tf.reduce_mean(tf.square(output_layer - X))
optimizer = tf.train.AdamOptimizer(learning_rate)
train = optimizer.minimize(loss)

# Initializing SAE:
init = tf.global_variables_initializer()

# Saving Model:
saver = tf.train.Saver()

# Running Session:
num_epochs = 5
batch_size = 150

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(num_epochs):
        num_batches = mnist.train.num_examples//batch_size
        for iteration in range(num_batches):
            X_batch, y_batch = mnist.train.next_batch(batch_size)
            sess.run(train, feed_dict={X:X_batch})
        
        training_loss = loss.eval(feed_dict={X:X_batch})
        print("EPOCH: {}  Loss: {}".format(epoch, training_loss))
    
    saver.save(sess, './example_stacked_autoencoder.ckpt')

# Testing our SAE:
num_test_images = 10
with tf.Session() as sess:
    saver.restore(sess, './example_stacked_autoencoder.ckpt')
    results = otput_layer.eval(feed_dict = {X:mnist.test.images[ :num_test_images]})

print(results)
